# -*- coding: utf-8 -*-
"""Mini-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_i7zerAlHL_KfQ8LUImzSUhVgNbxyDNv
"""

!pip install wikipedia faiss-cpu sentence-transformers transformers torch numpy langchain_community
import os
os.environ["WANDB_DISABLED"] = "true"


from langchain_community.document_loaders import WikipediaLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

"""**Load Wikipedia**"""

loader = WikipediaLoader(
    query="Artificial Intelligence",
    load_max_docs=2
)

documents = loader.load()
print(f"Loaded {len(documents)} Wikipedia documents")


#Chunking function

def chunk_text(text, size=400, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

chunks = []
for doc in documents:
    chunks.extend(chunk_text(doc.page_content))

print(f"Total chunks: {len(chunks)}")

"""**Create Embeddings**"""

embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True)

"""**FAISS Vector Store**"""

import faiss
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings)

"""**Load Pretrained Seq2Seq Generator**"""

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""**RAG Inference**"""



"""**Ask Question**"""

question = "What is artificial intelligence?"
answer, evidence = rag_answer(question)

print("\nQuestion:", question)
print("\nAnswer:", answer)

print("\nRetrieved Evidence:")
for i, e in enumerate(evidence, 1):
    print(f"\nSource {i}:")
    print(e[:300])

