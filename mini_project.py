# -*- coding: utf-8 -*-
"""Mini Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_i7zerAlHL_KfQ8LUImzSUhVgNbxyDNv
"""

!pip install wikipedia faiss-cpu sentence-transformers transformers torch numpy langchain_community
import os
os.environ["WANDB_DISABLED"] = "true"


from langchain_community.document_loaders import WikipediaLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

"""**Load Wikipedia**"""

loader = WikipediaLoader(
    query="Artificial Intelligence",
    load_max_docs=2
)

documents = loader.load()
print(f"Loaded {len(documents)} Wikipedia documents")


#Chunking function

def chunk_text(text, size=400, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

chunks = []
for doc in documents:
    chunks.extend(chunk_text(doc.page_content))

print(f"Total chunks: {len(chunks)}")

"""**Create Embeddings**"""

embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True)

"""**FAISS Vector Store**"""

import faiss
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings)

"""**Load Pretrained Seq2Seq Generator**"""

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""**RAG Inference**"""

def rag_answer(question, top_k=3):
    query_vec = embedder.encode([question])
    distances, indices = index.search(query_vec, top_k)

    retrieved_chunks = [chunks[i] for i in indices[0]]
    context = "\n".join(retrieved_chunks)

    prompt = f"""
Answer the question using the context below.

Context:
{context}

Question:
{question}
"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    outputs = model.generate(**inputs, max_new_tokens=150)

    return tokenizer.decode(outputs[0], skip_special_tokens=True), retrieved_chunks

"""**Ask Question**"""

question = "What is artificial intelligence?"
answer, evidence = rag_answer(question)

print("\nQuestion:", question)
print("\nAnswer:", answer)

print("\nRetrieved Evidence:")
for i, e in enumerate(evidence, 1):
    print(f"\nSource {i}:")
    print(e[:300])

